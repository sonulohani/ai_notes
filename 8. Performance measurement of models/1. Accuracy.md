## Classification: Accuracy

Accuracy is one metric for evaluating classification models. Informally, accuracy is the fraction of predictions our model got right. It is the ratio of number of correct predictions to the total number of input samples. Formally, accuracy has the following definition:

$Accuracy = \frac{Number of correct predictions}{Total number of predictions}$

It works well only if there are equal number of samples belonging to each class.

For example, consider that there are 98% samples of class A and 2% samples of class B in our training set. This is the case of imbalanced dataset. Then our model can easily get 98% training accuracy by simply predicting every training sample belonging to class A. This model is dumb model.

When the same model is tested on a test set with 60% samples of class A and 40% samples of class B, then the test accuracy would drop down to 60%. Classification Accuracy is great, but gives us the false sense of achieving high accuracy.

The real problem arises, when the cost of misclassification of the minor class samples are very high. If we deal with a rare but fatal disease, the cost of failing to diagnose the disease of a sick person is much higher than the cost of sending a healthy person to more tests.

You should never use accuracy when using imbalanced dataset.

Reference 1:- https://developers.google.com/machine-learning/crash-course/classification/accuracy

Reference 2:- https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234