## Best and Worst cases in KNN

### Best
1. When dimensions is small(<10) [not large] $\leftarrow$ KNN is the best algorithm.
2. **K-NN is pretty intuitive and simple:** K-NN algorithm is very simple to understand and equally easy to implement. To classify the new data point K-NN algorithm reads through whole dataset to find out K nearest neighbors.
3. **K-NN has no assumptions:** K-NN is a non-parametric algorithm which means there are assumptions to be met to implement K-NN. Parametric models like linear regression has lots of assumptions to be met by data before it can be implemented which is not the case with K-NN.
4. **No Training Step:** K-NN does not explicitly build any model, it simply tags the new data entry based learning from historical data. New data entry would be tagged with majority class in the nearest neighbor.
5. **It constantly evolves:** Given it’s an instance-based learning; k-NN is a memory-based approach. The classifier immediately adapts as we collect new training data. It allows the algorithm to respond quickly to changes in the input during real-time use.
6. **Very easy to implement for multi-class problem:** Most of the classifier algorithms are easy to implement for binary problems and needs effort to implement for multi class whereas K-NN adjust to multi class without any extra efforts.
7. **Can be used both for Classification and Regression:** One of the biggest advantages of K-NN is that K-NN can be used both for classification and regression problems.
8. **One Hyper Parameter:** K-NN might take some time while selecting the first hyper parameter but after that rest of the parameters are aligned to it.
9. **Variety of distance criteria to be choose from:** K-NN algorithm gives user the flexibility to choose distance while building K-NN model. BTW if you know the right distance measure for you data, then KNN is the best algorithm.

- Euclidean Distance
- Hamming Distance
- Manhattan Distance
- Minkowski Distance
- Cosine Distance

### Worst
1. **K-NN slow algorithm:** K-NN might be very easy to implement but as dataset grows efficiency or speed of algorithm declines very fast. When dimensions is large, interpretibility of the model decreases. Run-time complexity in kd tree increases.
2. **Curse of Dimensionality:** KNN works well with small number of input variables but as the numbers of variables grow K-NN algorithm struggles to predict the output of new data point.
3. **Not for low latency applications:** KNN shouldn't be used in low latency application(internet applications).
4. **K-NN needs homogeneous features:** If you decide to build k-NN using a common distance, like Euclidean or Manhattan distances, it is completely necessary that features have the same scale, since absolute differences in features weight the same, i.e., a given distance in feature 1 must means the same for feature 5. **Optimal number of neighbors:** One of the biggest issues with K-NN is to choose the optimal number of neighbors to be consider while classifying the new data entry.
6. **Imbalanced data causes problems:** k-NN doesn’t perform well on imbalanced data. If we consider two classes, A and B, and the majority of the training data is labeled as A, then the model will ultimately give a lot of preference to A. This might result in getting the less common class B wrongly classified.
7. **Outlier sensitivity:** K-NN algorithm is very sensitive to outliers as it simply chose the neighbors based on distance criteria.
8. **Missing Value treatment:** K-NN inherently has no capability of dealing with missing value problem.

Reference:- https://www.fromthegenesis.com/pros-and-cons-of-k-nearest-neighbors/